# Fix: historial de partidas no se actualiza por tama√±o (GitHub)

## üöÄ AI IMPLEMENTATION PROMPT (entrada para otra IA)

Eres una IA de desarrollo y debes **implementar** mejoras para evitar que el historial de partidas deje de actualizarse por l√≠mites de tama√±o al guardar en GitHub.

### Objetivo
Garantizar que **todos** los jugadores (incluyendo los que tienen much√≠simas partidas) puedan persistir y leer su historial sin fallar por:
- L√≠mite de tama√±o de GitHub Contents API.
- Overhead de Base64 al subir el contenido.

### Contexto t√©cnico del repo
- Lenguaje: Python (Flask).
- Persistencia: archivos en GitHub v√≠a Contents API desde el m√≥dulo services/github_service.py.
- Existe formato legacy y formato ‚Äúv2 weekly‚Äù ya implementado:
  - Legacy: match_history/{puuid}.json
  - v2: match_history/{puuid}/index.json + match_history/{puuid}/weeks/{YYYY}-W{WW}.json

### Requisitos (obligatorios)
1) Mantener compatibilidad de lectura:
   - Si existe match_history/{puuid}/index.json (v2/v3) se debe poder leer y reconstruir la lista de matches.
   - Si no existe index, usar legacy match_history/{puuid}.json.

2) Evitar fallos por tama√±o:
   - Nunca intentar subir un payload que exceda el l√≠mite pr√°ctico.
   - Considerar que el upload via Contents API usa Base64 (overhead ~33%).

3) Implementar ‚Äúv3 chunks por tama√±o‚Äù:
   - Una semana ISO puede dividirse en m√∫ltiples archivos si su tama√±o excede el umbral.
   - Ejemplo de nombres v√°lidos:
     - weeks/2026-W07-01.json
     - weeks/2026-W07-02.json
   - El index debe listar TODOS los chunks reales en el campo files.

4) Actualizar consumidores que asumen match_history/*.json plano:
   - El script validate_lp_assignments.py hoy itera match_history/*.json y asume estructura {"matches": [...]}
   - Debe soportar tambi√©n el formato por carpetas con index.json + files.

5) Integridad al escribir:
   - Escribir chunks primero y actualizar index al final.
   - El index NO debe referenciar chunks que no se hayan guardado correctamente.

### Alcance
- Cambios de backend/servicios y scripts.
- No cambiar UX, templates, endpoints, ni dise√±o.

### No-alcance (no hacer)
- No a√±adir p√°ginas nuevas ni features de UI.
- No migrar masivamente ni borrar legacy autom√°ticamente (solo si se indica en secci√≥n ‚ÄúOpcional‚Äù).

### Archivos a modificar (m√≠nimo)
1) services/github_service.py
   - save_player_match_history(puuid, historial_data)
   - (opcional recomendado) write_file_to_github(...)
2) validate_lp_assignments.py

### Implementaci√≥n esperada (pasos concretos)

#### Paso A ‚Äî Umbral correcto considerando Base64
En services/github_service.py:
- Implementar una funci√≥n utilitaria para estimar el tama√±o del payload:
  - bytes_json = len(json_str.encode('utf-8'))
  - bytes_b64 = len(base64.b64encode(json_str.encode('utf-8')))
- Definir un umbral conservador, por ejemplo:
  - MAX_B64_BYTES = 950_000  (ajustable)
  - O equivalente en JSON bytes si prefieres.

#### Paso B ‚Äî Split dentro de semana (v3)
En save_player_match_history():
- Agrupar matches por semana ISO como ya est√°.
- Para cada semana:
  - Serializar la lista a JSON compacto.
  - Si supera el umbral, partir week_matches en N chunks.
  - Guardar cada chunk como weeks/{wk}-{NN}.json
  - A√±adir cada path relativo a index.files.

Reglas:
- Cada chunk debe mantener orden por game_end_timestamp DESC.
- Debe ser determinista: con los mismos matches, el orden de files debe ser estable.
- Si una semana no excede el umbral, puede seguir guard√°ndose como weeks/{wk}.json (compat v2) o tambi√©n como -01. Elige UNA pol√≠tica y documenta.

#### Paso C ‚Äî Index consistente
- Solo incluir en files los chunks que realmente se guardaron OK.
- Si falla alg√∫n chunk, no romper el index previo: devolver False y dejar el estado consistente.

#### Paso D ‚Äî Logging defensivo
En write_file_to_github():
- Loguear:
  - bytes_json
  - bytes_b64
  - status_code y response.text truncado (ya existe)

#### Paso F ‚Äî Robustez (recomendado)
Sin cambiar la UX ni los endpoints, mejorar tolerancia a fallos:
- Si GitHub devuelve conflicto por SHA (p.ej. 409/422 dependiendo del caso):
  - Re-leer SHA y reintentar 1-2 veces con backoff corto.
- Si GitHub devuelve rate limit/forbidden (403) o errores temporales (5xx):
  - Reintentar con backoff exponencial corto (m√°x 2-3 intentos) y luego fallar de forma limpia.
- Asegurar que un fallo parcial no deje el index apuntando a chunks inexistentes.

#### Paso E ‚Äî Actualizar validate_lp_assignments.py
- Cambiar validate_match_lp_assignments() para que:
  - Recorra match_history/.
  - Si encuentra archivos *.json con estructura legacy, procesarlos como hoy.
  - Si encuentra subcarpetas (cada una para un puuid):
    - Leer index.json
    - Leer cada archivo listado en index.files
    - Unir matches y validar igual.

### Criterios de aceptaci√≥n (Definition of Done)
- Un jugador con historial grande:
  - No falla al guardar en GitHub.
  - Se crean m√∫ltiples archivos por semana si hace falta.
  - index.json lista los chunks correctos.
- Un jugador peque√±o:
  - Sigue funcionando (legacy o weekly), sin errores.
- Lectura:
  - read_player_match_history() reconstruye matches correctamente desde index + files.
- validate_lp_assignments.py:
  - Puede validar tanto legacy como formato por carpetas.

### Validaci√≥n recomendada
- Ejecutar un ciclo que intente guardar un historial artificial ‚Äúgrande‚Äù y verificar que se parte.
- Ejecutar validate_lp_assignments.py en un directorio match_history que contenga ambos formatos.

### Entregables
- C√≥digo modificado en los archivos listados.
- Si introduces un nuevo formato (v3), actualizar el √≠ndice para reflejarlo (sin romper v2).

---

## Contexto y diagn√≥stico (referencia)

## Contexto
En este proyecto el historial de partidas por jugador se persiste en el repo v√≠a GitHub API (Contents API), usando [`services/github_service.py`](services/github_service.py).

- Lectura: `read_player_match_history(puuid)`
- Escritura: `save_player_match_history(puuid, historial_data)`

Ya existe un formato **v2** ‚Äúpor semanas‚Äù para evitar el archivo √∫nico grande:
- Legacy: `match_history/{puuid}.json`
- v2 weekly: `match_history/{puuid}/index.json` + `match_history/{puuid}/weeks/{YYYY}-W{WW}.json`

## S√≠ntoma
Jugadores con muchas partidas dejan de actualizarse porque el archivo (o chunk semanal) supera el l√≠mite de tama√±o aceptado por la API de GitHub.

## Causa ra√≠z (probable)
1) **L√≠mite de la GitHub Contents API**: el endpoint `PUT /repos/{owner}/{repo}/contents/{path}` tiene l√≠mites pr√°cticos (‚âà 1MB de contenido). Cuando el archivo excede ese l√≠mite, GitHub responde con error (p.ej. 413 / 422 seg√∫n el caso).

2) **Overhead por Base64**: `write_file_to_github()` sube el contenido en Base64. El tama√±o que ‚Äúviaja‚Äù en la request crece ~33%.
   - En `save_player_match_history()` se usa `MAX_CONTENTS_BYTES = 900_000` (estimando bytes del JSON UTF‚Äë8).
   - Pero 900KB de JSON ‚Üí ~1.2MB base64, lo que puede fallar aunque el JSON ‚Äúparezca‚Äù < 1MB.

3) **Semana demasiado grande**: incluso con v2 semanal, una semana con much√≠simas partidas puede seguir superando el l√≠mite.

## Objetivo
Garantizar que **siempre** se pueda persistir historial de partidas, incluso para jugadores con much√≠simas partidas, sin romper la lectura existente.

## Estrategia recomendada (v3: chunks por tama√±o)
Mantener el ‚Äúindex + lista de archivos‚Äù (v2), pero permitir que una semana se divida en **sub‚Äëchunks** por tama√±o.

### Idea
En lugar de guardar solo:
- `weeks/2026-W07.json`

Permitir:
- `weeks/2026-W07-01.json`
- `weeks/2026-W07-02.json`
- `weeks/2026-W07-03.json`

Y en `index.json` mantener `files: [...]` con TODOS los paths relativos.

‚úÖ Ventaja importante: **`read_player_match_history()` ya concatena `files`**, as√≠ que si `files` contiene 3 archivos para una semana, la lectura seguir√° funcionando **sin cambios** (solo concatenar√° m√°s partes).

## Impacto en otros procesos (qu√© hay que cambiar/revisar)

En esta codebase, casi todo el consumo del historial pasa por `get_player_match_history()` ‚Üí `read_player_match_history()`. Eso **ya soporta** el formato ‚Äúindex + files‚Äù (v2) y por lo tanto tambi√©n soportar√° ‚Äúweeks con sufijo -NN‚Äù (v3) siempre que se mantenga la lista `files` en el index.

### Consumidores que NO deber√≠an romperse (porque usan el servicio)
- `services/match_service.py` (lectura/escritura centralizada).
- `services/data_updater.py` (workers de actualizaci√≥n que leen/guardan v√≠a `read_player_match_history` / `save_player_match_history`).
- Blueprints: `blueprints/main.py`, `blueprints/player.py`, `blueprints/stats.py`, `blueprints/api.py` (usan `get_player_match_history`).
- Generaci√≥n del index: `services/index_json_generator.py` (usa `get_player_match_history(puuid, limit=20)`).

### Consumidores que S√ç requieren cambios si el historial se parte en carpetas/archivos

#### 1) Scripts locales que iteran `match_history/*.json`
Ejemplo: `validate_lp_assignments.py`.

Actualmente asume que dentro de `match_history/` solo hay archivos `*.json` con estructura `{"matches": [...]}`.
Con formato v2/v3, para jugadores grandes habr√° carpetas:
- `match_history/{puuid}/index.json`
- `match_history/{puuid}/weeks/*.json`

Qu√© cambiar:
- Si encuentra un archivo `match_history/{puuid}.json` (legacy), procesarlo como hoy.
- Si encuentra una carpeta `match_history/{puuid}/`:
  - Cargar `index.json`.
  - Iterar `files` y cargar cada archivo listado.
  - Combinar en una √∫nica lista `matches` para ejecutar la validaci√≥n igual que antes.

#### 2) Herramientas externas / usos fuera del c√≥digo
Si existe cualquier job externo (otro repo, un script en CI, o una persona) que descargue `match_history/{puuid}.json` directamente desde GitHub, eso **ya no ser√° confiable** para jugadores grandes (porque se guardar√°n en v2/v3).

Qu√© hacer:
- Documentar que la fuente ‚Äúoficial‚Äù es `match_history/{puuid}/index.json` cuando exista.
- (Opcional) generar un ‚Äúlegacy recortado‚Äù (√∫ltimas N partidas) en `match_history/{puuid}.json` para compatibilidad humana/externa.

#### 3) Estad√≠sticas globales
Las stats globales en runtime se calculan desde listas `all_matches` armadas a partir de `get_player_match_history()`, as√≠ que **no deber√≠an romperse** por el split.

Lo que s√≠ conviene revisar:
- Performance: al combinar muchos chunks, leer un historial completo (`limit=-1`) har√° muchas requests a GitHub.
  - Mitigaci√≥n: mantener el uso de `limit=20` donde sea posible (como ya hace el index).
  - Para c√°lculos globales: evitar recalcular full-scan demasiado seguido (ya existe `GLOBAL_STATS_UPDATE_INTERVAL`).

### Ajuste clave: umbral por Base64
Cambiar la l√≥gica de ‚Äúumbral‚Äù para que se base en el tama√±o real aproximado de la carga:
- Opci√≥n A (simple): bajar el umbral del JSON UTF‚Äë8, p.ej. `MAX_JSON_BYTES = 650_000`.
- Opci√≥n B (mejor): calcular el tama√±o Base64 y comparar contra un m√°ximo conservador.

Recomendaci√≥n: usar Opci√≥n B si se toca `write_file_to_github()`; si no, usar Opci√≥n A en `save_player_match_history()`.

## Cambios puntuales a realizar

### 1) `services/github_service.py`

#### 1.1 Ajustar umbral
En `save_player_match_history()`:
- Cambiar `MAX_CONTENTS_BYTES = 900_000` por un valor m√°s conservador (p.ej. 650_000) **o** calcular bytes base64.

Motivo: evitar fallos por el overhead de base64.

#### 1.2 Split por tama√±o dentro de semana
En el loop que guarda cada `week_matches`:
- Si `week_bytes > MAX_*`, dividir `week_matches` en partes:
  - `weeks/{wk}-01.json`, `weeks/{wk}-02.json`, ...
- Agregar cada parte a `files`.

Puntos a cuidar:
- Mantener orden (m√°s reciente primero) en cada chunk.
- Evitar duplicados: si ya hay archivos antiguos para esa semana, decidir pol√≠tica (ver ‚ÄúMigraci√≥n‚Äù).

#### 1.3 (Opcional pero recomendable) Mejorar `write_file_to_github()`
En `write_file_to_github()` hoy se loguea tama√±o de `content_json` (UTF‚Äë8), pero no el de Base64.
- Agregar log de `len(content_b64)`
- (Opcional) si supera un m√°ximo, devolver `False` antes de llamar a GitHub.

### 2) Migraci√≥n / convivencia con legacy

Hay dos escenarios:

**Escenario A (sin migraci√≥n masiva, recomendado para m√≠nimo riesgo):**
- Dejar los legacy `match_history/{puuid}.json` como est√°n.
- Para los jugadores grandes, a partir de ahora se guarda v2/v3 en carpeta.
- `read_player_match_history()` ya prioriza index v2 cuando existe.

**Escenario B (migraci√≥n controlada):**
- Ejecutar un script que para cada `puuid`:
  - Lee legacy.
  - Llama a `save_player_match_history()` para escribir en v2/v3.
  - (Opcional) borra legacy despu√©s de verificar.

Recomendaci√≥n: solo borrar legacy si est√°s seguro de que no hay consumidores externos.

## Validaci√≥n (c√≥mo saber que qued√≥ bien)

### Validaci√≥n funcional
- Para un jugador ‚Äúchico‚Äù: se sigue guardando legacy (o v2), y `historial_global` / vista de jugador muestra partidas.
- Para un jugador ‚Äúgrande‚Äù: se guarda en m√∫ltiples archivos; `index.json` lista varios.

### Validaci√≥n de tama√±o
- Confirmar en logs de `write_file_to_github()`:
  - `bytes(JSON)` y `bytes(Base64)` quedan bajo el m√°ximo.

### Validaci√≥n de lectura
- `read_player_match_history()` debe:
  - Leer `index.json`.
  - Descargar todos los paths de `files`.
  - Combinar y ordenar por `game_end_timestamp`.

## Checklist de implementaci√≥n

- [ ] 1. Reproducir el fallo con un jugador con muchas partidas (log de error HTTP de GitHub).
- [ ] 2. Confirmar tama√±o del payload (JSON + base64) al momento de fallar.
- [ ] 3. Ajustar umbral en `save_player_match_history()` (bajar a ~650KB o medir base64).
- [ ] 4. Implementar split por tama√±o dentro de la semana (`weeks/{wk}-NN.json`).
- [ ] 5. Verificar que `index.json` incluya todos los chunks (ordenados recientes‚Üíantiguos).
- [ ] 6. Probar guardado de:
  - [ ] jugador peque√±o (1 archivo)
  - [ ] jugador mediano (semanal simple)
  - [ ] jugador grande (semanal dividido en N partes)
- [ ] 7. Probar lectura para los 3 casos y validar orden/duplicados.
- [ ] 8. (Opcional) A√±adir logs defensivos en `write_file_to_github()` para tama√±o base64.
- [ ] 9. Desplegar y monitorear: buscar respuestas no-200/201 en `write_file_to_github`.
- [ ] 10. Actualizar scripts locales que leen `match_history/*.json` (p.ej. `validate_lp_assignments.py`) para soportar carpetas con `index.json` + `files`.
- [ ] 11. (Opcional) Plan de migraci√≥n controlada para pasar legacy‚Üív2/v3.
- [ ] 12. (Opcional) Mantener compatibilidad externa: generar `match_history/{puuid}.json` recortado (√∫ltimas N) si hay consumidores fuera del c√≥digo.

## Notas operativas
- Si `GITHUB_TOKEN` no est√° configurado, nada se guarda (ver `write_file_to_github`).
- El proceso de actualizaci√≥n en background est√° en `services/data_updater.py` y termina llamando a `save_player_match_history()`.

## Consideraciones adicionales (para no llevarse sorpresas)

### L√≠mites y comportamiento de GitHub
- **Contents API y tama√±o**: incluso si el JSON pesa < 1MB, la request puede fallar por el overhead de Base64 y por l√≠mites pr√°cticos del endpoint.
- **Rate limits**:
  - Sin token o con token con permisos limitados, GitHub puede aplicar rate limit con facilidad.
  - El formato v2/v3 implica **m√°s requests** (index + N chunks). Para lecturas completas (`limit=-1`) el n√∫mero de requests crece linealmente con el n√∫mero de chunks.
- **Latencia y timeouts**: `read_file_from_github()` usa timeouts relativamente cortos (raw 30s, API 30s). Con muchos chunks, aumentan las probabilidades de fallos intermitentes.

### Consistencia/atomicidad al escribir (chunks + index)
- El flujo recomendado es: **guardar chunks primero** y **al final** escribir `index.json`.
- Riesgo: si se guardan algunos chunks pero falla el index, esos chunks quedan ‚Äúhu√©rfanos‚Äù (no referenciados por `files`). No es grave funcionalmente, pero hace crecer el repo.
- Riesgo inverso (peor): si se actualiza el index apuntando a chunks que no se llegaron a escribir, la lectura quedar√≠a incompleta.
  - Por eso el index debe incluir **solo** los archivos que se guardaron correctamente.
- Recomendaci√≥n: en caso de fallo parcial, dejar el index anterior intacto y reintentar la escritura en el siguiente ciclo.

### Concurrencia (m√∫ltiples hilos/procesos)
- Si dos workers intentan persistir el mismo jugador a la vez (o dos instancias de la app), pueden pisarse:
  - Ambos leen el mismo SHA y hacen PUT ‚Üí uno puede fallar con conflicto.
  - Ambos pueden generar index con listas `files` diferentes.
- Mitigaciones posibles:
  - Garantizar un √∫nico escritor por PUUID (lock en memoria o cola de trabajos).
  - Reintentos con backoff cuando GitHub devuelva conflicto.

### Duplicados y orden
- Al combinar chunks, el orden final se normaliza ordenando por `game_end_timestamp`.
- Si se re-procesa una partida y se vuelve a insertar por error, el split por chunks no lo evita.
  - Recomendaci√≥n: mantener un set de `match_id` al construir `matches` para evitar duplicados antes de persistir.

### Rendimiento y costo de lectura
- `get_player_match_history(limit=20)` es barato y deber√≠a usarse donde sea posible (como ya hace el index).
- `limit=-1` + v2/v3 puede ser costoso (muchos downloads). Considerar:
  - Optimizar lectura: cargar primero los chunks m√°s recientes y parar cuando se alcance el l√≠mite.
  - Cachear en memoria resultados parciales por jugador si el endpoint se consulta mucho.

### Crecimiento del repositorio
- Partir historial en muchos archivos hace que el repo crezca r√°pido (cada update agrega/actualiza blobs).
- Recomendaciones de operaci√≥n:
  - Evitar re-escrituras completas innecesarias; solo tocar semanas afectadas.
  - Considerar rotaci√≥n (p.ej. mantener temporada actual, archivar temporadas pasadas) si el repo empieza a pesar demasiado.

### Compatibilidad hacia atr√°s y hacia fuera
- Dentro de la app: mientras se use `get_player_match_history()`, el cambio deber√≠a ser transparente.
- Fuera de la app:
  - Si alguien consume `match_history/{puuid}.json` directamente, ese path puede dejar de estar actualizado para jugadores grandes.
  - Mitigaci√≥n opcional: mantener un legacy ‚Äúrecortado‚Äù (√∫ltimas N partidas) para compatibilidad.

### Rollback
- Si se necesitara revertir, se puede volver a leer desde v2/v3 y re-generar un legacy con las √∫ltimas N partidas.
- No conviene depender de un rollback que regenere el JSON completo si el problema original era el tama√±o.

### Observabilidad
- Asegurar logs √∫tiles en `write_file_to_github()`:
  - status code + primeros ~500 chars de error (ya existe)
  - tama√±o JSON y, si se a√±ade, tama√±o Base64
- Monitorizar espec√≠ficamente:
  - `413/422` (tama√±o) y `409` (conflicto SHA)
  - `403` (rate limit o permisos)

### Seguridad
- `GITHUB_TOKEN` debe tener permisos suficientes para escribir (scope t√≠pico `repo` si es privado).
- Evitar loguear tokens o URLs que los contengan.

### Alternativas si GitHub se queda corto
- Si el hist√≥rico sigue creciendo (o hay demasiadas requests), GitHub deja de ser ideal como ‚Äústorage‚Äù. Alternativas t√≠picas:
  - Objeto en S3/R2/GCS
  - DB simple (SQLite/Postgres)
  - Cache + job de snapshot con retenci√≥n

## Resultado esperado
Despu√©s del cambio, ning√∫n jugador deber√≠a dejar de actualizar el historial por tama√±o: el sistema partir√° autom√°ticamente el historial en archivos suficientemente peque√±os para GitHub.
